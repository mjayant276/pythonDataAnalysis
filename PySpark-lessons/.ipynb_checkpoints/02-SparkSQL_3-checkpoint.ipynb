{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ace15a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Jayanth\\\\Python_DataAnalysis\\\\PySpark-lessons'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c36c2c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e23e9de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('WindowFunctions').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00de536b",
   "metadata": {},
   "source": [
    "- Window.partitionBy: This can be used to categorize data and perform analysis within sub categories. Comma separated multiple columns can be added in window partition. Example: In order to calculate sum of salary for each Department along with individual detail, partition by dept number is required. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d49e5e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-------+----------+-------+-----+\n",
      "| emp_no|emp_name| salary|manager_id|dept_no| comm|\n",
      "+-------+--------+-------+----------+-------+-----+\n",
      "|1000245| PRADEEP|5000.00|      null|    100| 0.00|\n",
      "|1000258|   BLAKE|2850.00|   1000245|    300|50.00|\n",
      "|1000262|   CLARK|2450.00|   1000245|    100|50.00|\n",
      "|1000276|   JONES|2975.00|   1000245|    200|75.00|\n",
      "|1000288|   SCOTT|3000.00|   1000276|    200| 0.00|\n",
      "|1000292|    FORD|3000.00|   1000276|    200| 0.00|\n",
      "|1000294|   SMITH| 800.00|   1000292|    200| 0.00|\n",
      "|1000299|   ALLEN|1600.00|   1000258|    300| 0.00|\n",
      "|1000310|    WARD|1250.00|   1000258|    300|50.00|\n",
      "|1000312|  MARTIN|1250.00|   1000258|    300|50.00|\n",
      "|1000315|  TURNER|1500.00|   1000258|    300| 0.00|\n",
      "|1000326|   ADAMS|1100.00|   1000288|    200| 0.00|\n",
      "|1000336|   JAMES| 950.00|   1000258|    300|50.00|\n",
      "|1000346|  MILLER|1300.00|   1000262|    100| 0.00|\n",
      "|1000347|   DAVID|1400.00|   1000245|    500| 0.00|\n",
      "+-------+--------+-------+----------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(\"file:///C:\\\\Users\\\\Jayanth\\\\Python_DataAnalysis\\\\PySpark-lessons\\\\datasets\\\\emp_data.parquet\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3291eb3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- emp_no: integer (nullable = true)\n",
      " |-- emp_name: string (nullable = true)\n",
      " |-- salary: decimal(18,2) (nullable = true)\n",
      " |-- manager_id: integer (nullable = true)\n",
      " |-- dept_no: integer (nullable = true)\n",
      " |-- comm: decimal(5,2) (nullable = true)\n",
      "\n",
      "+-------+--------+-------+----------+-------+-----+\n",
      "|emp_no |emp_name|salary |manager_id|dept_no|comm |\n",
      "+-------+--------+-------+----------+-------+-----+\n",
      "|1000245|PRADEEP |5000.00|null      |100    |0.00 |\n",
      "|1000258|BLAKE   |2850.00|1000245   |300    |50.00|\n",
      "|1000262|CLARK   |2450.00|1000245   |100    |50.00|\n",
      "|1000276|JONES   |2975.00|1000245   |200    |75.00|\n",
      "|1000288|SCOTT   |3000.00|1000276   |200    |0.00 |\n",
      "|1000292|FORD    |3000.00|1000276   |200    |0.00 |\n",
      "|1000294|SMITH   |800.00 |1000292   |200    |0.00 |\n",
      "|1000299|ALLEN   |1600.00|1000258   |300    |0.00 |\n",
      "|1000310|WARD    |1250.00|1000258   |300    |50.00|\n",
      "|1000312|MARTIN  |1250.00|1000258   |300    |50.00|\n",
      "|1000315|TURNER  |1500.00|1000258   |300    |0.00 |\n",
      "|1000326|ADAMS   |1100.00|1000288   |200    |0.00 |\n",
      "|1000336|JAMES   |950.00 |1000258   |300    |50.00|\n",
      "|1000346|MILLER  |1300.00|1000262   |100    |0.00 |\n",
      "|1000347|DAVID   |1400.00|1000245   |500    |0.00 |\n",
      "+-------+--------+-------+----------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d8787e",
   "metadata": {},
   "source": [
    "- Window.orderBy:can be used to specify order in which rows should be processed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80c66c9",
   "metadata": {},
   "source": [
    "- count function(): count function can be used to count number of records for each group. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96540e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+-------+----------+\n",
      "|emp_name|dept_no| salary|Total_Emps|\n",
      "+--------+-------+-------+----------+\n",
      "| PRADEEP|    100|5000.00|         3|\n",
      "|   CLARK|    100|2450.00|         3|\n",
      "|  MILLER|    100|1300.00|         3|\n",
      "|   JONES|    200|2975.00|         5|\n",
      "|   SCOTT|    200|3000.00|         5|\n",
      "|    FORD|    200|3000.00|         5|\n",
      "|   SMITH|    200| 800.00|         5|\n",
      "|   ADAMS|    200|1100.00|         5|\n",
      "|   BLAKE|    300|2850.00|         6|\n",
      "|   ALLEN|    300|1600.00|         6|\n",
      "|    WARD|    300|1250.00|         6|\n",
      "|  MARTIN|    300|1250.00|         6|\n",
      "|  TURNER|    300|1500.00|         6|\n",
      "|   JAMES|    300| 950.00|         6|\n",
      "|   DAVID|    500|1400.00|         1|\n",
      "+--------+-------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import  Window\n",
    "from pyspark.sql.functions import count, col\n",
    "\n",
    "empdf = df.select(\"emp_name\", \"dept_no\", \"salary\", count(\"*\").over(Window.partitionBy(\"dept_no\")).alias(\"Total_Emps\"))\n",
    "empdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1b508f",
   "metadata": {},
   "source": [
    "**Window.rowsBetween:** Can be used when user want to peek into other rows data to compute current row value. \n",
    "   - rowsBetween( Window.unboundedPreceding|Window.currentRow|Window.currentRow+-n (start window) , Window.currentRow+-m |Window.currentRow| Window.unboundedFollowing (end window))\n",
    "   - rowsBetween attribute require user to specify start window and end window of row. \n",
    "   - Window.unboundedPreceding, Window.unboundedFollowing and Window.currentRow values are available to peek into previous rows, following rows and current row respectively. \n",
    "   - User can add or subtract number(e.g 1,2,3) from Window.currentRow to check that many number of previous / following rows or specify UNBOUNDED attributes to check all the previous / following rows. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6278d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+-------+-------------+\n",
      "|emp_name|dept_no| salary|Remaining_cnt|\n",
      "+--------+-------+-------+-------------+\n",
      "| PRADEEP|    100|5000.00|            2|\n",
      "|   CLARK|    100|2450.00|            1|\n",
      "|  MILLER|    100|1300.00|            0|\n",
      "|   JONES|    200|2975.00|            4|\n",
      "|   SCOTT|    200|3000.00|            3|\n",
      "|    FORD|    200|3000.00|            2|\n",
      "|   SMITH|    200| 800.00|            1|\n",
      "|   ADAMS|    200|1100.00|            0|\n",
      "|   BLAKE|    300|2850.00|            5|\n",
      "|   ALLEN|    300|1600.00|            4|\n",
      "|    WARD|    300|1250.00|            3|\n",
      "|  MARTIN|    300|1250.00|            2|\n",
      "|  TURNER|    300|1500.00|            1|\n",
      "|   JAMES|    300| 950.00|            0|\n",
      "|   DAVID|    500|1400.00|            0|\n",
      "+--------+-------+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empdf = df.select(\"emp_name\", \"dept_no\", \"salary\", count(\"*\").over(Window.partitionBy(\"dept_no\").rowsBetween(Window.currentRow+1, Window.unboundedFollowing)).alias(\"Remaining_cnt\"))\n",
    "empdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458762fb",
   "metadata": {},
   "source": [
    "**sum function():** sum function can be used to calculate sum of each column passed to this function for each group. This function can be applied to only numeric columns.\n",
    "\n",
    "- Cumulative sum of salaries grouped over department number(dept_no). There will be problem when order by values are same within a partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba178337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+-------+---------------+\n",
      "|emp_name|dept_no| salary|cummulative_sum|\n",
      "+--------+-------+-------+---------------+\n",
      "| PRADEEP|    100|5000.00|        5000.00|\n",
      "|   CLARK|    100|2450.00|        7450.00|\n",
      "|  MILLER|    100|1300.00|        8750.00|\n",
      "|   SCOTT|    200|3000.00|        6000.00|\n",
      "|    FORD|    200|3000.00|        6000.00|\n",
      "+--------+-------+-------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import  Window\n",
    "from pyspark.sql.functions import sum, col\n",
    "\n",
    "empdf = df.select(\"emp_name\", \"dept_no\", \"salary\", sum(\"salary\").over(Window.partitionBy(\"dept_no\").orderBy( col(\"salary\").desc())).alias(\"cummulative_sum\"))\n",
    "empdf.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053d1e36",
   "metadata": {},
   "source": [
    "- True cumulative sum of salaries within each department, use multiple order by columns to help spark distinguish rows. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e6f995b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+-------+--------------------+\n",
      "|emp_name|dept_no| salary|true_cummulative_sum|\n",
      "+--------+-------+-------+--------------------+\n",
      "| PRADEEP|    100|5000.00|             5000.00|\n",
      "|   CLARK|    100|2450.00|             7450.00|\n",
      "|  MILLER|    100|1300.00|             8750.00|\n",
      "|   SCOTT|    200|3000.00|             3000.00|\n",
      "|    FORD|    200|3000.00|             6000.00|\n",
      "|   JONES|    200|2975.00|             8975.00|\n",
      "|   ADAMS|    200|1100.00|            10075.00|\n",
      "|   SMITH|    200| 800.00|            10875.00|\n",
      "|   BLAKE|    300|2850.00|             2850.00|\n",
      "|   ALLEN|    300|1600.00|             4450.00|\n",
      "+--------+-------+-------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import  Window\n",
    "from pyspark.sql.functions import sum, col\n",
    "\n",
    "empdf = df.select(\"emp_name\", \"dept_no\", \"salary\", sum(\"salary\").over(Window.partitionBy(\"dept_no\").orderBy( col(\"salary\").desc(), col(\"emp_name\").desc())).alias(\"true_cummulative_sum\"))\n",
    "empdf.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6933fd0e",
   "metadata": {},
   "source": [
    "- Cumulative sum of salaries without any partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3af5612d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+-------+--------------------+\n",
      "|emp_name|dept_no| salary|true_cummulative_sum|\n",
      "+--------+-------+-------+--------------------+\n",
      "| PRADEEP|    100|5000.00|             5000.00|\n",
      "|   SCOTT|    200|3000.00|             8000.00|\n",
      "|    FORD|    200|3000.00|            11000.00|\n",
      "|   JONES|    200|2975.00|            13975.00|\n",
      "|   BLAKE|    300|2850.00|            16825.00|\n",
      "|   CLARK|    100|2450.00|            19275.00|\n",
      "|   ALLEN|    300|1600.00|            20875.00|\n",
      "|  TURNER|    300|1500.00|            22375.00|\n",
      "|   DAVID|    500|1400.00|            23775.00|\n",
      "|  MILLER|    100|1300.00|            25075.00|\n",
      "|    WARD|    300|1250.00|            26325.00|\n",
      "|  MARTIN|    300|1250.00|            27575.00|\n",
      "|   ADAMS|    200|1100.00|            28675.00|\n",
      "|   JAMES|    300| 950.00|            29625.00|\n",
      "|   SMITH|    200| 800.00|            30425.00|\n",
      "+--------+-------+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import  Window\n",
    "from pyspark.sql.functions import sum, col\n",
    "\n",
    "empdf = df.select(\"emp_name\", \"dept_no\", \"salary\", sum(\"salary\").over(Window.orderBy( col(\"salary\").desc(), col(\"emp_name\").desc())).alias(\"true_cummulative_sum\"))\n",
    "\n",
    "empdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb84c43",
   "metadata": {},
   "source": [
    "- Sum of salaries within each department "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "39ec9c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+-------+--------------------+\n",
      "|emp_name|dept_no| salary|true_cummulative_sum|\n",
      "+--------+-------+-------+--------------------+\n",
      "| PRADEEP|    100|5000.00|             8750.00|\n",
      "|   CLARK|    100|2450.00|             8750.00|\n",
      "|  MILLER|    100|1300.00|             8750.00|\n",
      "|   SCOTT|    200|3000.00|            10875.00|\n",
      "|    FORD|    200|3000.00|            10875.00|\n",
      "|   JONES|    200|2975.00|            10875.00|\n",
      "|   ADAMS|    200|1100.00|            10875.00|\n",
      "|   SMITH|    200| 800.00|            10875.00|\n",
      "|   BLAKE|    300|2850.00|             9400.00|\n",
      "|   ALLEN|    300|1600.00|             9400.00|\n",
      "|  TURNER|    300|1500.00|             9400.00|\n",
      "|    WARD|    300|1250.00|             9400.00|\n",
      "|  MARTIN|    300|1250.00|             9400.00|\n",
      "|   JAMES|    300| 950.00|             9400.00|\n",
      "|   DAVID|    500|1400.00|             1400.00|\n",
      "+--------+-------+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import  Window\n",
    "from pyspark.sql.functions import sum, col\n",
    "\n",
    "empdf = df.select(\"emp_name\", \"dept_no\", \"salary\", sum(\"salary\").over(Window.partitionBy(\"dept_no\").orderBy( col(\"salary\").desc(), col(\"emp_name\").desc()).rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)).alias(\"true_cummulative_sum\"))\n",
    "\n",
    "empdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dee115",
   "metadata": {},
   "source": [
    "**avg function():** sum function can be used to calculate sum of each column passed to this function for each group. This function can be applied to only numeric columns.\n",
    "- Average salary within each department \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b68b74fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+-------+-------------------+\n",
      "|emp_name|dept_no| salary|Average_dept_salary|\n",
      "+--------+-------+-------+-------------------+\n",
      "| PRADEEP|    100|5000.00|        2916.666667|\n",
      "|   CLARK|    100|2450.00|        2916.666667|\n",
      "|  MILLER|    100|1300.00|        2916.666667|\n",
      "|   JONES|    200|2975.00|        2175.000000|\n",
      "|   SCOTT|    200|3000.00|        2175.000000|\n",
      "|    FORD|    200|3000.00|        2175.000000|\n",
      "|   SMITH|    200| 800.00|        2175.000000|\n",
      "|   ADAMS|    200|1100.00|        2175.000000|\n",
      "|   BLAKE|    300|2850.00|        1566.666667|\n",
      "|   ALLEN|    300|1600.00|        1566.666667|\n",
      "+--------+-------+-------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import  Window\n",
    "from pyspark.sql.functions import avg, col\n",
    "\n",
    "empdf = df.select(\"emp_name\", \"dept_no\", \"salary\", avg(\"salary\").over(Window.partitionBy(\"dept_no\")).alias(\"Average_dept_salary\"))\n",
    "empdf.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d15d0d7",
   "metadata": {},
   "source": [
    "- Cumulative Average salary within each department "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8a195d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+-------+-------------------+\n",
      "|emp_name|dept_no| salary|Cum_avg_Dept_salary|\n",
      "+--------+-------+-------+-------------------+\n",
      "|   CLARK|    100|2450.00|        2450.000000|\n",
      "|  MILLER|    100|1300.00|        1875.000000|\n",
      "| PRADEEP|    100|5000.00|        2916.666667|\n",
      "|   ADAMS|    200|1100.00|        1100.000000|\n",
      "|    FORD|    200|3000.00|        2050.000000|\n",
      "|   JONES|    200|2975.00|        2358.333333|\n",
      "|   SCOTT|    200|3000.00|        2518.750000|\n",
      "|   SMITH|    200| 800.00|        2175.000000|\n",
      "|   ALLEN|    300|1600.00|        1600.000000|\n",
      "|   BLAKE|    300|2850.00|        2225.000000|\n",
      "|   JAMES|    300| 950.00|        1800.000000|\n",
      "|  MARTIN|    300|1250.00|        1662.500000|\n",
      "|  TURNER|    300|1500.00|        1630.000000|\n",
      "|    WARD|    300|1250.00|        1566.666667|\n",
      "|   DAVID|    500|1400.00|        1400.000000|\n",
      "+--------+-------+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import  Window\n",
    "from pyspark.sql.functions import avg, col\n",
    "\n",
    "empdf = df.select(\"emp_name\", \"dept_no\", \"salary\", avg(\"salary\").over(Window.partitionBy(\"dept_no\").orderBy(\"emp_name\").rowsBetween(Window.unboundedPreceding, Window.currentRow)).alias(\"Cum_avg_Dept_salary\"))\n",
    "\n",
    "\n",
    "empdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf1af7d",
   "metadata": {},
   "source": [
    "- Specifying the data as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2539438f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+--------+-------+-------+\n",
      "|dept_no|        avg|     sum|    min|    max|\n",
      "+-------+-----------+--------+-------+-------+\n",
      "|    100|2916.666667| 8750.00|1300.00|5000.00|\n",
      "|    200|2175.000000|10875.00| 800.00|3000.00|\n",
      "|    300|1566.666667| 9400.00| 950.00|2850.00|\n",
      "|    500|1400.000000| 1400.00|1400.00|1400.00|\n",
      "+-------+-----------+--------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "windowSpecAgg  = Window.partitionBy(\"dept_no\")\n",
    "from pyspark.sql.functions import col,avg,sum,min,max,row_number \n",
    "df.withColumn(\"row\",row_number().over(windowSpec)) \\\n",
    "  .withColumn(\"avg\", avg(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .withColumn(\"sum\", sum(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .withColumn(\"min\", min(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .withColumn(\"max\", max(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .where(col(\"row\")==1).select(\"dept_no\",\"avg\",\"sum\",\"min\",\"max\") \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "513b0e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "windowSpec  = Window.partitionBy(\"dept_no\").orderBy(\"salary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f61717c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-------+----------+-------+-----+----------+\n",
      "|emp_no |emp_name|salary |manager_id|dept_no|comm |row_number|\n",
      "+-------+--------+-------+----------+-------+-----+----------+\n",
      "|1000346|MILLER  |1300.00|1000262   |100    |0.00 |1         |\n",
      "|1000262|CLARK   |2450.00|1000245   |100    |50.00|2         |\n",
      "|1000245|PRADEEP |5000.00|null      |100    |0.00 |3         |\n",
      "|1000294|SMITH   |800.00 |1000292   |200    |0.00 |1         |\n",
      "|1000326|ADAMS   |1100.00|1000288   |200    |0.00 |2         |\n",
      "|1000276|JONES   |2975.00|1000245   |200    |75.00|3         |\n",
      "|1000288|SCOTT   |3000.00|1000276   |200    |0.00 |4         |\n",
      "|1000292|FORD    |3000.00|1000276   |200    |0.00 |5         |\n",
      "|1000336|JAMES   |950.00 |1000258   |300    |50.00|1         |\n",
      "|1000310|WARD    |1250.00|1000258   |300    |50.00|2         |\n",
      "|1000312|MARTIN  |1250.00|1000258   |300    |50.00|3         |\n",
      "|1000315|TURNER  |1500.00|1000258   |300    |0.00 |4         |\n",
      "|1000299|ALLEN   |1600.00|1000258   |300    |0.00 |5         |\n",
      "|1000258|BLAKE   |2850.00|1000245   |300    |50.00|6         |\n",
      "|1000347|DAVID   |1400.00|1000245   |500    |0.00 |1         |\n",
      "+-------+--------+-------+----------+-------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"row_number\",row_number().over(windowSpec)) \\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1b355c",
   "metadata": {},
   "source": [
    "- rank of the salary for each department"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc9cb153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-------+----------+-------+-----+----+\n",
      "| emp_no|emp_name| salary|manager_id|dept_no| comm|rank|\n",
      "+-------+--------+-------+----------+-------+-----+----+\n",
      "|1000346|  MILLER|1300.00|   1000262|    100| 0.00|   1|\n",
      "|1000262|   CLARK|2450.00|   1000245|    100|50.00|   2|\n",
      "|1000245| PRADEEP|5000.00|      null|    100| 0.00|   3|\n",
      "|1000294|   SMITH| 800.00|   1000292|    200| 0.00|   1|\n",
      "|1000326|   ADAMS|1100.00|   1000288|    200| 0.00|   2|\n",
      "|1000276|   JONES|2975.00|   1000245|    200|75.00|   3|\n",
      "|1000288|   SCOTT|3000.00|   1000276|    200| 0.00|   4|\n",
      "|1000292|    FORD|3000.00|   1000276|    200| 0.00|   4|\n",
      "|1000336|   JAMES| 950.00|   1000258|    300|50.00|   1|\n",
      "|1000310|    WARD|1250.00|   1000258|    300|50.00|   2|\n",
      "|1000312|  MARTIN|1250.00|   1000258|    300|50.00|   2|\n",
      "|1000315|  TURNER|1500.00|   1000258|    300| 0.00|   4|\n",
      "|1000299|   ALLEN|1600.00|   1000258|    300| 0.00|   5|\n",
      "|1000258|   BLAKE|2850.00|   1000245|    300|50.00|   6|\n",
      "|1000347|   DAVID|1400.00|   1000245|    500| 0.00|   1|\n",
      "+-------+--------+-------+----------+-------+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import rank\n",
    "df.withColumn(\"rank\",rank().over(windowSpec)) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cddf9d7",
   "metadata": {},
   "source": [
    "- Unique row number against employees by salary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e304a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+-------+-----------------+\n",
      "|emp_name|dept_no| salary|row_number_salary|\n",
      "+--------+-------+-------+-----------------+\n",
      "|   SMITH|    200| 800.00|                1|\n",
      "|   JAMES|    300| 950.00|                2|\n",
      "|   ADAMS|    200|1100.00|                3|\n",
      "|    WARD|    300|1250.00|                4|\n",
      "|  MARTIN|    300|1250.00|                5|\n",
      "|  MILLER|    100|1300.00|                6|\n",
      "|   DAVID|    500|1400.00|                7|\n",
      "|  TURNER|    300|1500.00|                8|\n",
      "|   ALLEN|    300|1600.00|                9|\n",
      "|   CLARK|    100|2450.00|               10|\n",
      "|   BLAKE|    300|2850.00|               11|\n",
      "|   JONES|    200|2975.00|               12|\n",
      "|   SCOTT|    200|3000.00|               13|\n",
      "|    FORD|    200|3000.00|               14|\n",
      "| PRADEEP|    100|5000.00|               15|\n",
      "+--------+-------+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import  Window\n",
    "from pyspark.sql.functions import row_number, col\n",
    "\n",
    "empdf_update = df.select(\"emp_name\", \"dept_no\", \"salary\", row_number().over(Window.orderBy(\"salary\")).alias(\"row_number_salary\"))\n",
    "empdf_update.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61559a2",
   "metadata": {},
   "source": [
    "- rank function(): This function is used to find rank within partitions or sub-partitions. Rank function will miss next number(rank) if there are 2 records with same value.\n",
    "- Row number against employees by salary within department. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d347e727",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+-------+----------------------+\n",
      "|emp_name|dept_no| salary|row_number_Dept_salary|\n",
      "+--------+-------+-------+----------------------+\n",
      "|  MILLER|    100|1300.00|                     1|\n",
      "|   CLARK|    100|2450.00|                     2|\n",
      "| PRADEEP|    100|5000.00|                     3|\n",
      "|   SMITH|    200| 800.00|                     1|\n",
      "|   ADAMS|    200|1100.00|                     2|\n",
      "|   JONES|    200|2975.00|                     3|\n",
      "|   SCOTT|    200|3000.00|                     4|\n",
      "|    FORD|    200|3000.00|                     5|\n",
      "|   JAMES|    300| 950.00|                     1|\n",
      "|    WARD|    300|1250.00|                     2|\n",
      "|  MARTIN|    300|1250.00|                     3|\n",
      "|  TURNER|    300|1500.00|                     4|\n",
      "|   ALLEN|    300|1600.00|                     5|\n",
      "|   BLAKE|    300|2850.00|                     6|\n",
      "|   DAVID|    500|1400.00|                     1|\n",
      "+--------+-------+-------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import  Window\n",
    "from pyspark.sql.functions import row_number, col\n",
    "\n",
    "empdf_update = df.select(\"emp_name\", \"dept_no\", \"salary\", row_number().over(Window.partitionBy(\"dept_no\").orderBy(\"salary\")).alias(\"row_number_Dept_salary\"))\n",
    "empdf_update.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d1aeea",
   "metadata": {},
   "source": [
    "-  Rank of employees by salary within department. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "004a115f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+-------+--------------+\n",
      "|emp_name|dept_no|salary |Rank_by_salary|\n",
      "+--------+-------+-------+--------------+\n",
      "|PRADEEP |100    |5000.00|1             |\n",
      "|CLARK   |100    |2450.00|2             |\n",
      "|MILLER  |100    |1300.00|3             |\n",
      "|SCOTT   |200    |3000.00|1             |\n",
      "|FORD    |200    |3000.00|1             |\n",
      "|JONES   |200    |2975.00|3             |\n",
      "|ADAMS   |200    |1100.00|4             |\n",
      "|SMITH   |200    |800.00 |5             |\n",
      "|BLAKE   |300    |2850.00|1             |\n",
      "|ALLEN   |300    |1600.00|2             |\n",
      "|TURNER  |300    |1500.00|3             |\n",
      "|WARD    |300    |1250.00|4             |\n",
      "|MARTIN  |300    |1250.00|4             |\n",
      "|JAMES   |300    |950.00 |6             |\n",
      "|DAVID   |500    |1400.00|1             |\n",
      "+--------+-------+-------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import  Window\n",
    "from pyspark.sql.functions import rank, col\n",
    "\n",
    "empdf_update = df.select(\"emp_name\", \"dept_no\", \"salary\", rank().over(Window.partitionBy(\"dept_no\").orderBy( col(\"salary\").desc())).alias(\"Rank_by_salary\"))\n",
    "empdf_update.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c62211",
   "metadata": {},
   "source": [
    "#### Creating a table name with emp_data_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23f42980",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_table_name = \"emp_data_csv\"\n",
    "df.createOrReplaceTempView(temp_table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "974972b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "permanent_table_name = \"emp_data_table\"\n",
    "df.write.format(\"parquet\").saveAsTable(permanent_table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3623656e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-------+----------+-------+-----+\n",
      "| emp_no|emp_name| salary|manager_id|dept_no| comm|\n",
      "+-------+--------+-------+----------+-------+-----+\n",
      "|1000245| PRADEEP|5000.00|      null|    100| 0.00|\n",
      "|1000258|   BLAKE|2850.00|   1000245|    300|50.00|\n",
      "|1000262|   CLARK|2450.00|   1000245|    100|50.00|\n",
      "|1000276|   JONES|2975.00|   1000245|    200|75.00|\n",
      "|1000288|   SCOTT|3000.00|   1000276|    200| 0.00|\n",
      "|1000292|    FORD|3000.00|   1000276|    200| 0.00|\n",
      "|1000294|   SMITH| 800.00|   1000292|    200| 0.00|\n",
      "|1000299|   ALLEN|1600.00|   1000258|    300| 0.00|\n",
      "|1000310|    WARD|1250.00|   1000258|    300|50.00|\n",
      "|1000312|  MARTIN|1250.00|   1000258|    300|50.00|\n",
      "|1000315|  TURNER|1500.00|   1000258|    300| 0.00|\n",
      "|1000326|   ADAMS|1100.00|   1000288|    200| 0.00|\n",
      "|1000336|   JAMES| 950.00|   1000258|    300|50.00|\n",
      "|1000346|  MILLER|1300.00|   1000262|    100| 0.00|\n",
      "|1000347|   DAVID|1400.00|   1000245|    500| 0.00|\n",
      "+-------+--------+-------+----------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from emp_data_table\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2aad7b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-------+----------+-------+-----+-----------+\n",
      "| emp_no|emp_name| salary|manager_id|dept_no| comm| avg_salary|\n",
      "+-------+--------+-------+----------+-------+-----+-----------+\n",
      "|1000346|  MILLER|1300.00|   1000262|    100| 0.00|1300.000000|\n",
      "|1000262|   CLARK|2450.00|   1000245|    100|50.00|1875.000000|\n",
      "|1000245| PRADEEP|5000.00|      null|    100| 0.00|2916.666667|\n",
      "|1000294|   SMITH| 800.00|   1000292|    200| 0.00| 800.000000|\n",
      "|1000326|   ADAMS|1100.00|   1000288|    200| 0.00| 950.000000|\n",
      "|1000276|   JONES|2975.00|   1000245|    200|75.00|1625.000000|\n",
      "|1000288|   SCOTT|3000.00|   1000276|    200| 0.00|2175.000000|\n",
      "|1000292|    FORD|3000.00|   1000276|    200| 0.00|2175.000000|\n",
      "|1000336|   JAMES| 950.00|   1000258|    300|50.00| 950.000000|\n",
      "|1000310|    WARD|1250.00|   1000258|    300|50.00|1150.000000|\n",
      "|1000312|  MARTIN|1250.00|   1000258|    300|50.00|1150.000000|\n",
      "|1000315|  TURNER|1500.00|   1000258|    300| 0.00|1237.500000|\n",
      "|1000299|   ALLEN|1600.00|   1000258|    300| 0.00|1310.000000|\n",
      "|1000258|   BLAKE|2850.00|   1000245|    300|50.00|1566.666667|\n",
      "|1000347|   DAVID|1400.00|   1000245|    500| 0.00|1400.000000|\n",
      "+-------+--------+-------+----------+-------+-----+-----------+\n",
      "\n",
      "None\n",
      "+-------+--------+-------+----------+-------+-----+-----------+----------+\n",
      "| emp_no|emp_name| salary|manager_id|dept_no| comm| avg_salary|max_salary|\n",
      "+-------+--------+-------+----------+-------+-----+-----------+----------+\n",
      "|1000346|  MILLER|1300.00|   1000262|    100| 0.00|1300.000000|   5000.00|\n",
      "|1000262|   CLARK|2450.00|   1000245|    100|50.00|1875.000000|   5000.00|\n",
      "|1000245| PRADEEP|5000.00|      null|    100| 0.00|2916.666667|   5000.00|\n",
      "|1000294|   SMITH| 800.00|   1000292|    200| 0.00| 800.000000|   3000.00|\n",
      "|1000326|   ADAMS|1100.00|   1000288|    200| 0.00| 950.000000|   3000.00|\n",
      "|1000276|   JONES|2975.00|   1000245|    200|75.00|1625.000000|   3000.00|\n",
      "|1000288|   SCOTT|3000.00|   1000276|    200| 0.00|2175.000000|   3000.00|\n",
      "|1000292|    FORD|3000.00|   1000276|    200| 0.00|2175.000000|   3000.00|\n",
      "|1000336|   JAMES| 950.00|   1000258|    300|50.00| 950.000000|   2850.00|\n",
      "|1000310|    WARD|1250.00|   1000258|    300|50.00|1150.000000|   2850.00|\n",
      "|1000312|  MARTIN|1250.00|   1000258|    300|50.00|1150.000000|   2850.00|\n",
      "|1000315|  TURNER|1500.00|   1000258|    300| 0.00|1237.500000|   2850.00|\n",
      "|1000299|   ALLEN|1600.00|   1000258|    300| 0.00|1310.000000|   2850.00|\n",
      "|1000258|   BLAKE|2850.00|   1000245|    300|50.00|1566.666667|   2850.00|\n",
      "|1000347|   DAVID|1400.00|   1000245|    500| 0.00|1400.000000|   1400.00|\n",
      "+-------+--------+-------+----------+-------+-----+-----------+----------+\n",
      "\n",
      "None\n",
      "+-------+--------+-------+----------+-------+-----+-----------+----------+----------+\n",
      "| emp_no|emp_name| salary|manager_id|dept_no| comm| avg_salary|max_salary|min_salary|\n",
      "+-------+--------+-------+----------+-------+-----+-----------+----------+----------+\n",
      "|1000346|  MILLER|1300.00|   1000262|    100| 0.00|1300.000000|   5000.00|   1300.00|\n",
      "|1000262|   CLARK|2450.00|   1000245|    100|50.00|1875.000000|   5000.00|   1300.00|\n",
      "|1000245| PRADEEP|5000.00|      null|    100| 0.00|2916.666667|   5000.00|   1300.00|\n",
      "|1000294|   SMITH| 800.00|   1000292|    200| 0.00| 800.000000|   3000.00|    800.00|\n",
      "|1000326|   ADAMS|1100.00|   1000288|    200| 0.00| 950.000000|   3000.00|    800.00|\n",
      "|1000276|   JONES|2975.00|   1000245|    200|75.00|1625.000000|   3000.00|    800.00|\n",
      "|1000288|   SCOTT|3000.00|   1000276|    200| 0.00|2175.000000|   3000.00|    800.00|\n",
      "|1000292|    FORD|3000.00|   1000276|    200| 0.00|2175.000000|   3000.00|    800.00|\n",
      "|1000336|   JAMES| 950.00|   1000258|    300|50.00| 950.000000|   2850.00|    950.00|\n",
      "|1000310|    WARD|1250.00|   1000258|    300|50.00|1150.000000|   2850.00|    950.00|\n",
      "|1000312|  MARTIN|1250.00|   1000258|    300|50.00|1150.000000|   2850.00|    950.00|\n",
      "|1000315|  TURNER|1500.00|   1000258|    300| 0.00|1237.500000|   2850.00|    950.00|\n",
      "|1000299|   ALLEN|1600.00|   1000258|    300| 0.00|1310.000000|   2850.00|    950.00|\n",
      "|1000258|   BLAKE|2850.00|   1000245|    300|50.00|1566.666667|   2850.00|    950.00|\n",
      "|1000347|   DAVID|1400.00|   1000245|    500| 0.00|1400.000000|   1400.00|   1400.00|\n",
      "+-------+--------+-------+----------+-------+-----+-----------+----------+----------+\n",
      "\n",
      "None\n",
      "+-------+--------+-------+----------+-------+-----+-----------+----------+----------+----------+\n",
      "| emp_no|emp_name| salary|manager_id|dept_no| comm| avg_salary|max_salary|min_salary|sum_salary|\n",
      "+-------+--------+-------+----------+-------+-----+-----------+----------+----------+----------+\n",
      "|1000346|  MILLER|1300.00|   1000262|    100| 0.00|1300.000000|   5000.00|   1300.00|   1300.00|\n",
      "|1000262|   CLARK|2450.00|   1000245|    100|50.00|1875.000000|   5000.00|   1300.00|   3750.00|\n",
      "|1000245| PRADEEP|5000.00|      null|    100| 0.00|2916.666667|   5000.00|   1300.00|   8750.00|\n",
      "|1000294|   SMITH| 800.00|   1000292|    200| 0.00| 800.000000|   3000.00|    800.00|    800.00|\n",
      "|1000326|   ADAMS|1100.00|   1000288|    200| 0.00| 950.000000|   3000.00|    800.00|   1900.00|\n",
      "|1000276|   JONES|2975.00|   1000245|    200|75.00|1625.000000|   3000.00|    800.00|   4875.00|\n",
      "|1000288|   SCOTT|3000.00|   1000276|    200| 0.00|2175.000000|   3000.00|    800.00|  10875.00|\n",
      "|1000292|    FORD|3000.00|   1000276|    200| 0.00|2175.000000|   3000.00|    800.00|  10875.00|\n",
      "|1000336|   JAMES| 950.00|   1000258|    300|50.00| 950.000000|   2850.00|    950.00|    950.00|\n",
      "|1000310|    WARD|1250.00|   1000258|    300|50.00|1150.000000|   2850.00|    950.00|   3450.00|\n",
      "|1000312|  MARTIN|1250.00|   1000258|    300|50.00|1150.000000|   2850.00|    950.00|   3450.00|\n",
      "|1000315|  TURNER|1500.00|   1000258|    300| 0.00|1237.500000|   2850.00|    950.00|   4950.00|\n",
      "|1000299|   ALLEN|1600.00|   1000258|    300| 0.00|1310.000000|   2850.00|    950.00|   6550.00|\n",
      "|1000258|   BLAKE|2850.00|   1000245|    300|50.00|1566.666667|   2850.00|    950.00|   9400.00|\n",
      "|1000347|   DAVID|1400.00|   1000245|    500| 0.00|1400.000000|   1400.00|   1400.00|   1400.00|\n",
      "+-------+--------+-------+----------+-------+-----+-----------+----------+----------+----------+\n",
      "\n",
      "None\n",
      "+-------+--------+-------+----------+-------+-----+-----------+----------+----------+----------+------------------+\n",
      "| emp_no|emp_name| salary|manager_id|dept_no| comm| avg_salary|max_salary|min_salary|sum_salary|count_salary_units|\n",
      "+-------+--------+-------+----------+-------+-----+-----------+----------+----------+----------+------------------+\n",
      "|1000346|  MILLER|1300.00|   1000262|    100| 0.00|1300.000000|   5000.00|   1300.00|   1300.00|                 3|\n",
      "|1000262|   CLARK|2450.00|   1000245|    100|50.00|1875.000000|   5000.00|   1300.00|   3750.00|                 3|\n",
      "|1000245| PRADEEP|5000.00|      null|    100| 0.00|2916.666667|   5000.00|   1300.00|   8750.00|                 3|\n",
      "|1000294|   SMITH| 800.00|   1000292|    200| 0.00| 800.000000|   3000.00|    800.00|    800.00|                 5|\n",
      "|1000326|   ADAMS|1100.00|   1000288|    200| 0.00| 950.000000|   3000.00|    800.00|   1900.00|                 5|\n",
      "|1000276|   JONES|2975.00|   1000245|    200|75.00|1625.000000|   3000.00|    800.00|   4875.00|                 5|\n",
      "|1000288|   SCOTT|3000.00|   1000276|    200| 0.00|2175.000000|   3000.00|    800.00|  10875.00|                 5|\n",
      "|1000292|    FORD|3000.00|   1000276|    200| 0.00|2175.000000|   3000.00|    800.00|  10875.00|                 5|\n",
      "|1000336|   JAMES| 950.00|   1000258|    300|50.00| 950.000000|   2850.00|    950.00|    950.00|                 6|\n",
      "|1000310|    WARD|1250.00|   1000258|    300|50.00|1150.000000|   2850.00|    950.00|   3450.00|                 6|\n",
      "|1000312|  MARTIN|1250.00|   1000258|    300|50.00|1150.000000|   2850.00|    950.00|   3450.00|                 6|\n",
      "|1000315|  TURNER|1500.00|   1000258|    300| 0.00|1237.500000|   2850.00|    950.00|   4950.00|                 6|\n",
      "|1000299|   ALLEN|1600.00|   1000258|    300| 0.00|1310.000000|   2850.00|    950.00|   6550.00|                 6|\n",
      "|1000258|   BLAKE|2850.00|   1000245|    300|50.00|1566.666667|   2850.00|    950.00|   9400.00|                 6|\n",
      "|1000347|   DAVID|1400.00|   1000245|    500| 0.00|1400.000000|   1400.00|   1400.00|   1400.00|                 1|\n",
      "+-------+--------+-------+----------+-------+-----+-----------+----------+----------+----------+------------------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Pyspark\n",
    "#reading data\n",
    "df = spark.sql(\"select * from emp_data_table\")\n",
    "\n",
    "\n",
    "# Aggregate Window Functions\n",
    "from pyspark.sql.functions import col,avg,sum,min,max,count\n",
    "from pyspark.sql import Window\n",
    "\n",
    "df = df.withColumn(\"avg_salary\", avg(col(\"salary\")).over(Window.partitionBy(\"dept_no\").orderBy(\"salary\")))\n",
    "print(df.show())\n",
    "df = df.withColumn(\"max_salary\", max(col(\"salary\")).over(Window.partitionBy(\"dept_no\")))\n",
    "print(df.show())\n",
    "df = df.withColumn(\"min_salary\", min(col(\"salary\")).over(Window.partitionBy(\"dept_no\")))\n",
    "print(df.show())\n",
    "df = df.withColumn(\"sum_salary\", sum(col(\"salary\")).over(Window.partitionBy(\"dept_no\").orderBy(\"salary\")))\n",
    "print(df.show())\n",
    "df = df.withColumn(\"count_salary_units\", count(col(\"salary\")).over(Window.partitionBy(\"dept_no\")))\n",
    "print(df.show())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034ec8e5",
   "metadata": {},
   "source": [
    "#### finding the row number, rank, dense rank, percent rank and ntile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fe834d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import row_number, rank, dense_rank, ntile, percent_rank\n",
    "from pyspark.sql import Window\n",
    "\n",
    "df = df.withColumn(\"row_number\", row_number().over(Window.partitionBy(\"dept_no\").orderBy(\"salary\")))\n",
    "df = df.withColumn(\"rank\", rank().over(Window.partitionBy(\"dept_no\").orderBy(\"salary\")))\n",
    "df = df.withColumn(\"dense_rank\", dense_rank().over(Window.partitionBy(\"dept_no\").orderBy(\"salary\")))\n",
    "df = df.withColumn(\"percent_rank\", percent_rank().over(Window.partitionBy(\"dept_no\").orderBy(\"salary\")))\n",
    "df = df.withColumn(\"ntile\", ntile(10).over(Window.partitionBy(\"dept_no\").orderBy(\"salary\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "062b52dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-------+-------+----------+----+----------+------------+-----+\n",
      "| emp_no|emp_name|dept_no| salary|row_number|rank|dense_rank|percent_rank|ntile|\n",
      "+-------+--------+-------+-------+----------+----+----------+------------+-----+\n",
      "|1000336|   JAMES|    300| 950.00|         1|   1|         1|         0.0|    1|\n",
      "|1000310|    WARD|    300|1250.00|         2|   2|         2|         0.2|    2|\n",
      "|1000312|  MARTIN|    300|1250.00|         3|   2|         2|         0.2|    3|\n",
      "|1000315|  TURNER|    300|1500.00|         4|   4|         3|         0.6|    4|\n",
      "|1000299|   ALLEN|    300|1600.00|         5|   5|         4|         0.8|    5|\n",
      "+-------+--------+-------+-------+----------+----+----------+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"emp_no\",\"emp_name\",\"dept_no\",\"salary\",\"row_number\",\"rank\",\"dense_rank\",\"percent_rank\",\"ntile\").distinct().filter(df.manager_id==\"1000258\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bd1a8a",
   "metadata": {},
   "source": [
    "#### LEAD, LAG, FIRST_VALUE, LAST_VALUE, NTH_VALUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7fd0ad98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------------------+-------+-------+\n",
      "|dept_no| salary|          cume_dist|    lag|   lead|\n",
      "+-------+-------+-------------------+-------+-------+\n",
      "|    300|1250.00|                0.5| 950.00|1250.00|\n",
      "|    300|1250.00|                0.5|1250.00|1500.00|\n",
      "|    300|1500.00| 0.6666666666666666|1250.00|1600.00|\n",
      "|    300|1600.00| 0.8333333333333334|1500.00|2850.00|\n",
      "|    300| 950.00|0.16666666666666666|   null|1250.00|\n",
      "+-------+-------+-------------------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Analytical Window Functions\n",
    "from pyspark.sql.functions import cume_dist, lag, lead\n",
    "from pyspark.sql import Window\n",
    "\n",
    "df = df.withColumn(\"cume_dist\", cume_dist().over(Window.partitionBy(\"dept_no\").orderBy(\"salary\")))\n",
    "df = df.withColumn(\"lag\", lag(\"salary\").over(Window.partitionBy(\"dept_no\").orderBy(\"salary\")))\n",
    "df = df.withColumn(\"lead\", lead(\"salary\").over(Window.partitionBy(\"dept_no\").orderBy(\"salary\")))\n",
    "\n",
    "#df.display()\n",
    "#df.distinct().display()\n",
    "df.select(\"dept_no\",\"salary\",\"cume_dist\",\"lag\",\"lead\").distinct().filter(df.manager_id==\"1000258\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96affc54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
