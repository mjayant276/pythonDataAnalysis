{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dbe1945",
   "metadata": {},
   "source": [
    "### Joins & Higher Order Functions in Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7363383c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5eaf756",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a spark Session\n",
    "### Spark Session is the gateway for creating a spark Program.\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"HOFs\").getOrCreate()\n",
    "## * all the cores on the machine local - local environment \n",
    "### local[2] - It should use 2 cores of the CPU for running the program\n",
    "### URL of the master - yarn://<ip-address>:<port>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fccddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "893a2a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "custDF = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"file:///C:\\\\Users\\\\Jayanth\\\\Python_DataAnalysis\\\\PySpark-lessons\\\\datasets\\\\Customers.csv\")\n",
    "salesDF = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"file:///C:\\\\Users\\\\Jayanth\\\\Python_DataAnalysis\\\\PySpark-lessons\\\\datasets\\\\Sales.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71aaa2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "custDF = custDF.withColumnRenamed('ID','customer_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5da04e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-------+--------------------+\n",
      "|customer_id|First Name|Last Name|Gender|Company|          Occupation|\n",
      "+-----------+----------+---------+------+-------+--------------------+\n",
      "|          1|    Joseph|  Perkins|  Male|Dynazzy|Community Outreac...|\n",
      "|          2|  Jennifer|  Alvarez|Female|   DabZ|Senior Quality En...|\n",
      "|          3|     Roger|    Black|  Male|Tagfeed|   Account Executive|\n",
      "+-----------+----------+---------+------+-------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "custDF.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35344f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+\n",
      "|customer_id|Food ID|\n",
      "+-----------+-------+\n",
      "|        537|      9|\n",
      "|         97|      4|\n",
      "|        658|      1|\n",
      "+-----------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salesDF = salesDF.withColumnRenamed('Customer ID','customer_id')\n",
    "salesDF.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2530a072",
   "metadata": {},
   "source": [
    "#### BroadCast Join Hint in Spark 2.x and 3.x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b3d530",
   "metadata": {},
   "source": [
    "- Working : \n",
    "   - Broadcast join is famous join for joining small table(dimension table) with big table(fact table) by avoiding costly data shuffling.\n",
    "- When to use Broadcast Join:\n",
    "  - Spark broadcast joins are perfect for joining a large DataFrame with a small DataFrame.\n",
    "  - Broadcast joins are easier to run on a cluster.\n",
    "  - Spark can “broadcast” a small DataFrame by sending all the data in that small DataFrame to all nodes in the cluster.\n",
    "  - After the broadcast, small DataFrame Spark can perform a join without shuffling any of the data in the large DataFrame.\n",
    "  - let’s check the below example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c713fb",
   "metadata": {},
   "source": [
    "The broadcasted dataset should fit in the driver as well as executor nodes. The driver first gets the dataset from the executor side and then broadcasts the datasets to all the worker nodes where the partitions for the larger dataset are present.</br>\n",
    "spark.sql.autoBroadcastJoinThreshold (Default Value 10485760(10 MB)</br>\n",
    "The property is configurable and has a max limit of 8GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0efac05c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+---------+--------------------+-------+\n",
      "|customer_id|First Name|Last Name|Gender|  Company|          Occupation|Food ID|\n",
      "+-----------+----------+---------+------+---------+--------------------+-------+\n",
      "|        537|    Cheryl|  Carroll|Female| Zoombeat|    Registered Nurse|      9|\n",
      "|         97|    Amanda|  Watkins|Female|      Ozu| Account Coordinator|      4|\n",
      "|        658|   Patrick|     Webb|  Male|Browsebug|Community Outreac...|      1|\n",
      "|        202|     Louis| Campbell|  Male|Rhynoodle|Account Represent...|      2|\n",
      "|        155|   Carolyn|     Diaz|Female| Gigazoom|Database Administ...|      9|\n",
      "+-----------+----------+---------+------+---------+--------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "broadcastJoin = custDF.hint(\"broadcast\").join(salesDF,\"customer_id\")\n",
    "broadcastJoin.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfc4047",
   "metadata": {},
   "source": [
    "#### Shuffle hash join spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3323d477",
   "metadata": {},
   "source": [
    "- Shuffle hash join shuffles the data based on join keys and then perform the join\n",
    "   - When the table is relatively large, the use of broadcast may cause driver- as well as executor-side memory issues, then shuffle Hash Join is the right choice. \n",
    "   - It is an expensive join as it involves both shuffling and hashing. Also, it requires memory and computation for maintaining a hash table.\n",
    "   - Shuffle Hash Join is performed in two steps :\n",
    "       - Shuffling: The data from the Join tables are partitioned based on the Join key. It does shuffle the data across partitions to have the same Join keys of the record assigned to the corresponding partitions.\n",
    "       - Hash Join: A classic single node Hash Join algorithm is performed for the data on each partition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b7eee2",
   "metadata": {},
   "source": [
    "It follows the classic map-reduce pattern:\n",
    "\n",
    "    First it maps through two tables(dataframes)\n",
    "    Uses the join keys as output key\n",
    "    Shuffles both dataframes by the output key, So that rows related to same keys from both tables will be moved on to same machine.\n",
    "    In reducer phase, join the two datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43506195",
   "metadata": {},
   "source": [
    "When to use:\n",
    "\n",
    "Shuffle hash join works well-\n",
    "1. when the dataframe are distributed evenly with the keys you are used to join and\n",
    "2. when dataframes has enough number of keys for parallelism.\n",
    "\n",
    "Shuffle Hash Join is a join where both dataframe are partitioned using same partitioner. Here join keys will fall in the same partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09190b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+------------+--------------------+-------+\n",
      "|customer_id|First Name|Last Name|Gender|     Company|          Occupation|Food ID|\n",
      "+-----------+----------+---------+------+------------+--------------------+-------+\n",
      "|        833|   Carolyn|     Dunn|Female|       Yadel|           Recruiter|      7|\n",
      "|        833|   Carolyn|     Dunn|Female|       Yadel|           Recruiter|      3|\n",
      "|        148|  Patricia|     Reid|Female|Shuffledrive|Analog Circuit De...|      5|\n",
      "|        737|  Patricia|      Cox|Female|     Camimbo| Geological Engineer|      3|\n",
      "|        540|   Jessica|    Hicks|Female|     Cogibox|Structural Analys...|      3|\n",
      "+-----------+----------+---------+------+------------+--------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "shuffleHashJoin = custDF.hint(\"shuffle_hash\").join(salesDF,\"customer_id\")\n",
    "shuffleHashJoin.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd18d2e3",
   "metadata": {},
   "source": [
    "#### SortMerge Join"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3e7a15",
   "metadata": {},
   "source": [
    "- Sort merge join perform the sort operation first and then merges the datasets\n",
    "   - Shuffle Sort-merge Join (SMJ) involves shuffling of data to get the same Join key with the same worker, and then performing the Sort-merge Join operation at the partition level in the worker nodes.\n",
    "   - This is Spark’s default join strategy, Since Spark 2.3 the default value of spark.sql.join.preferSortMergeJoin has been changed to true."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ac3b94",
   "metadata": {},
   "source": [
    "It has 3 phases:\n",
    "\n",
    "    Shuffle Phase: Both large tables will be repartitioned as per the Join keys across the partitions in the cluster.\n",
    "    Sort Phase: Sort the data within each partition parallelly.\n",
    "    Merge Phase: Join the sorted and partitioned data. It is merging the dataset by iterating over the elements and joining the rows having the same value for the Join keys."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db843116",
   "metadata": {},
   "source": [
    "#### Cartesian Product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e7e203",
   "metadata": {},
   "source": [
    "Cartesian Product is one type of join where two dataframe are joined using all rows.\n",
    "\n",
    "This join can be forced using shuffle_replicate_nl hint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ec384b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cartesianProduct = custDF.hint(\"shuffle_replicate_nl\").join(salesDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc927cb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cartesianProduct.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb868d37",
   "metadata": {},
   "source": [
    "#### Adaptive Query Execution\n",
    "- AQE takes care of the number of partitions and the number of broadcast join it makes\n",
    "- Spark checks the stage statistics and determines if there are any Skew joins and optimizes it by splitting the bigger partitions into smaller (matching partition size on other table/dataframe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c2e6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "### run the code before AQE\n",
    "df1 = custDF.groupBy(\"company\").count()\n",
    "df1.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3ff2a223",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.adaptive.enabled\",True)\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\",True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b6a362f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = custDF.groupBy(\"company\").count()\n",
    "df1.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36314e3",
   "metadata": {},
   "source": [
    "#### Higher Order Functionsm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490ec72c",
   "metadata": {},
   "source": [
    "**transform**\n",
    "\n",
    "Returns an array of elements after applying a transformation to each element in the input array.\n",
    "\n",
    "   - col: name of column or expression\n",
    "   - f: a function that is applied to each element of the input array.\n",
    "     Can take one of the following forms:\n",
    "     1. Unary `(x: Column) -> Column: …`\n",
    "     2. Binary `(x: Column, i: Column) -> Column…`, where the second argument is a 0-based index of the element.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e87198c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import transform,when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "911d724c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|     doubled|\n",
      "+------------+\n",
      "|[2, 4, 6, 8]|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1, [1, 2, 3, 4])], (\"key\", \"values\"))\n",
    "df.select(transform(\"values\", lambda x: x * 2).alias(\"doubled\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "10145cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alternate(x, i):\n",
    "\n",
    "    return when(i % 2 == 0, x).otherwise(-x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "36f410f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|    alternated|\n",
      "+--------------+\n",
      "|[1, -2, 3, -4]|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(transform(\"values\", alternate).alias(\"alternated\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fff02c",
   "metadata": {},
   "source": [
    "- Example 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "140236c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|orig_array|new_array|\n",
      "+----------+---------+\n",
      "| [1, 2, 3]|[2, 3, 4]|\n",
      "+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select array(1,2,3) as orig_array,transform(array(1,2,3),x->x+1) as new_array\").show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d71ae9d",
   "metadata": {},
   "source": [
    "- Example 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ab0022e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- col1: long (nullable = true)\n",
      " |-- col2: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      "\n",
      "+----+------------+\n",
      "|col1|        col2|\n",
      "+----+------------+\n",
      "|   3|   [1, 2, 3]|\n",
      "|   4|[5, 6, 7, 8]|\n",
      "+----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "arrayData = [(3,[1,2,3]),(4,[5,6,7,8])] \n",
    "\n",
    "df = spark.createDataFrame(data=arrayData, schema = ['col1','col2']) \n",
    "\n",
    "df.printSchema() \n",
    "\n",
    "df.createOrReplaceTempView(\"mytesttable\") \n",
    "\n",
    "spark.sql(\"select * from mytesttable\").show() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6898168",
   "metadata": {},
   "source": [
    "- example 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e3b60a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------+---------+\n",
      "|col1|orig_array  |new_array|\n",
      "+----+------------+---------+\n",
      "|3   |[1, 2, 3]   |[8]      |\n",
      "|4   |[5, 6, 7, 8]|[12]     |\n",
      "+----+------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select col1 ,col2 as orig_array,transform(filter(col2,(x,i)->i=2),x->x+5) as new_array from mytesttable\").show(truncate=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9933407",
   "metadata": {},
   "source": [
    "- example 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4929fbdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------+---------------+\n",
      "|col1|orig_array  |new_array      |\n",
      "+----+------------+---------------+\n",
      "|3   |[1, 2, 3]   |[4, 5, 6]      |\n",
      "|4   |[5, 6, 7, 8]|[9, 10, 11, 12]|\n",
      "+----+------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select col1,col2 as orig_array,transform(col2,(x,i)->x+col1) as new_array from mytesttable\").show(truncate=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "391966df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import * \n",
    "from pyspark.sql.types import * \n",
    "\n",
    "                                                     \n",
    "ws_schema = StructType() \\\n",
    ".add(\"ws_id\", StringType()) \\\n",
    ".add(\"source\", MapType(StringType(), StructType() \\\n",
    ".add(\"description\", StringType()) \\\n",
    ".add(\"id\", IntegerType()) \\\n",
    ".add(\"temp\", ArrayType(IntegerType()))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "65c19c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9fd90185",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertJson(json,schema):\n",
    "    reader = spark.read.schema(schema)\n",
    "\n",
    "    return reader.json(sc.parallelize([json]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "84fd574e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ws_id: string, source: map<string,struct<description:string,id:int,temp:array<int>>>]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ws_DF = convertJson( \"\"\"{\n",
    "\n",
    "\n",
    "    \"ws_id\": \"ws_southeast\",\n",
    "    \"source\": {\n",
    "        \"weather-station\": {\n",
    "        \"id\": 1,\n",
    "        \"description\": \"Weather station located at heathrow Airport\",                                          \n",
    "        \"temp\":[15,16,17,20,22,25,22,20,19,18,17,16]            \n",
    "      },\n",
    "      \"thermostat\": {\n",
    "        \"id\": 5,\n",
    "        \"description\": \"Thermometer reading at Kew Gardens\",\n",
    "        \"temp\": [15,15,15,18,21,22,21,21,20,18,17,17]\n",
    "      }\n",
    "    }\n",
    "  }\"\"\", ws_schema)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "57d882d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ws_id: string, source: map<string,struct<description:string,id:int,temp:array<int>>>]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|ws_id       |source                                                                                                                                                                                                                          |\n",
      "+------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|ws_southeast|{weather-station -> {Weather station located at heathrow Airport, 1, [15, 16, 17, 20, 22, 25, 22, 20, 19, 18, 17, 16]}, thermostat -> {Thermometer reading at Kew Gardens, 5, [15, 15, 15, 18, 21, 22, 21, 21, 20, 18, 17, 17]}}|\n",
      "+------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display(ws_DF)\n",
    "ws_DF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0ee8686e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------------+--------------------------------------------------------------------------------------------------+\n",
      "|ws_id       |key            |value                                                                                             |\n",
      "+------------+---------------+--------------------------------------------------------------------------------------------------+\n",
      "|ws_southeast|weather-station|{Weather station located at heathrow Airport, 1, [15, 16, 17, 20, 22, 25, 22, 20, 19, 18, 17, 16]}|\n",
      "|ws_southeast|thermostat     |{Thermometer reading at Kew Gardens, 5, [15, 15, 15, 18, 21, 22, 21, 21, 20, 18, 17, 17]}         |\n",
      "+------------+---------------+--------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exploded_DF = ws_DF.select(\"ws_id\", explode(\"source\") )\n",
    "exploded_DF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "845e37a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------------+---+------------------------------------------------+\n",
      "|ws_id       |key            |id |temp                                            |\n",
      "+------------+---------------+---+------------------------------------------------+\n",
      "|ws_southeast|weather-station|1  |[15, 16, 17, 20, 22, 25, 22, 20, 19, 18, 17, 16]|\n",
      "|ws_southeast|thermostat     |5  |[15, 15, 15, 18, 21, 22, 21, 21, 20, 18, 17, 17]|\n",
      "+------------+---------------+---+------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tempDataDF = exploded_DF.select(\"ws_id\",\"key\", col(\"value.id\").alias(\"id\"),\"value.temp\")\n",
    "tempDataDF.createOrReplaceTempView(\"temp_table\")\n",
    "\n",
    "spark.sql(\"select * from temp_table\").show(truncate=False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f801f7f9",
   "metadata": {},
   "source": [
    "- exists - Returns whether a predicate holds for one or more elements in the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a60c8ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8e7dd3db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|any_negative|\n",
      "+------------+\n",
      "|       false|\n",
      "|        true|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1, [1, 2, 3, 4]), (2, [3, -1, 0])],(\"key\", \"values\"))\n",
    "df.select(exists(\"values\", lambda x: x < 0).alias(\"any_negative\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bc14d2",
   "metadata": {},
   "source": [
    "- forall- Returns whether a predicate holds for every element in the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c045b46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|all_foo|\n",
      "+-------+\n",
      "|  false|\n",
      "|  false|\n",
      "|   true|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(\n",
    "    [(1, [\"bar\"]), (2, [\"foo\", \"bar\"]), (3, [\"foobar\", \"foo\"])],\n",
    "    (\"key\", \"values\")\n",
    ")\n",
    "df.select(forall(\"values\", lambda x: x.rlike(\"foo\")).alias(\"all_foo\")).show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0a0a3f97",
   "metadata": {},
   "source": [
    "References:\n",
    "    spark.apache.org\n",
    "    https://medium.com/@ghoshsiddharth25/apache-spark-join-strategies-deep-dive-26bf7e85db28\n",
    "    https://www.linkedin.com/pulse/pyspark-sql-tip-using-transform-higher-order-function-tom-reid/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
